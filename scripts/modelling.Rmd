---
title: "Modelling"
author: "Charith Wijewardena"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(tidymodels)
library(textrecipes)
library(themis)

load("C:/Users/chari/Documents/afp-data/clean_data.Rda")

# Create training/testing splits and resampling folds. ----

# Seed the random number generator.
set.seed(2022)

contacts_split <- contacts %>% 
  # Remove any information regarding gifts given this year.
  mutate(total_gifts = total_gifts - total_gifts_this_year,
         total_number_of_gifts = total_number_of_gifts - num_gifts_this_year) %>% 
  
  # Create the outcome that we want to predict.
  mutate(donated = case_when(total_gifts_this_year > 0 ~ "Donated",
                             TRUE ~ "Not donated")) %>% 
  
  # Select ID, predictors and outcome.
  select(supporter_id, 
         gender, 
         religion, 
         state, 
         lat, 
         lon, 
         comms_preference, 
         total_gifts,
         total_gifts_last_year,
         total_gifts_two_years_ago,
         total_number_of_gifts, 
         num_gifts_last_year, 
         num_gifts_two_years_ago, 
         previous_campaigns, 
         nonfin_action,
         prev_source_2,
         postcode,
         donated) %>% 
  na.omit() %>% 
  initial_split(strata = donated)

# Training and testing splits.
train <- training(contacts_split)
test <- testing(contacts_split)

# Create resampling folds.
set.seed(3245)
folds <- vfold_cv(train, strata = donated)

# Specify metric.
mset <- metric_set(sensitivity, specificity, roc_auc, accuracy)
```

```{r}
# Register parallel backend.
doParallel::registerDoParallel()
```

```{r}
set.seed(7824)

# Create recipe.
lin_rec <- recipe(donated ~ supporter_id + total_gifts_last_year + total_gifts + total_gifts_two_years_ago +
                    total_number_of_gifts + gender + lat + lon, 
                  data = train) %>% 
  update_role(supporter_id, new_role = "id") %>% 
  step_downsample(donated, under_ratio = 2) %>%
  step_log(total_gifts_last_year, total_gifts, total_gifts_two_years_ago, total_number_of_gifts, base = 10, offset = 1) %>% 
  step_dummy(gender)

# Specify model.
lin_spec <- logistic_reg() %>% 
  set_mode("classification") %>% 
  set_engine("glm")

# Specify workflow.
lin_wf <- workflow() %>% 
  add_recipe(lin_rec) %>% 
  add_model(lin_spec)

# Fit resamples.
cv <- lin_wf %>%
  fit_resamples(folds, 
                metrics = mset)

cv %>% 
  collect_metrics()
```

Sens, spec, roc_auc:

Logistic regression:

0.148, 0.989, 0.756 on just log of total gifts last year.
0.607, 0.845, 0.756 + downsampling under ratio = 2.
0.578, 0.898, 0.801 + total gifts
0.564, 0.922, 0.813  + total gifts two years ago
0.612, 0.918, 0.833 + total number of gifts
0.616, 0.918, 0.830 + number of gifts last year *(removed)*
0.610, 0.915, 0.849 + gender


```{r}
set.seed(2891)

# Create recipe.
xgb_rec <- recipe(donated ~ supporter_id + total_gifts_last_year + total_gifts + total_gifts_two_years_ago + total_number_of_gifts + gender + previous_campaigns + comms_preference + nonfin_action + postcode, 
                  data = train) %>% 
  update_role(supporter_id, new_role = "id") %>% 
  step_downsample(donated, under_ratio = 2) %>%
  step_log(total_gifts_last_year, total_gifts, total_gifts_two_years_ago, total_number_of_gifts, base = 10, offset = 1) %>% 
  step_dummy(gender, nonfin_action) %>% 
  step_tokenize(previous_campaigns, token = "regex", options = list(pattern = "|")) %>% 
  step_tokenfilter(previous_campaigns, max_tokens = 10) %>% 
  step_tokenize(comms_preference, token = "regex", options = list(pattern = ",")) %>% 
  step_tokenfilter(comms_preference, max_tokens = 3) %>%
  step_tf(previous_campaigns, comms_preference)

# Specify model.
xgb_spec <- boost_tree() %>% 
  set_mode("classification") %>% 
  set_engine("xgboost")

# Specify workflow.
xgb_wf <- workflow() %>% 
  add_recipe(xgb_rec) %>% 
  add_model(xgb_spec)

# Fit resamples.
xgb_cv <- xgb_wf %>%
  fit_resamples(folds,
                metrics = mset)
# xgb_cv <- xgb_wf %>%
#   tune_grid(folds,
#             grid = crossing(trees = seq(1, 25, 1)),
#             metrics = mset)

xgb_cv %>% 
  collect_metrics()

xgb_cv %>% 
  autoplot()
```

Sens, spec, roc_auc, accuracy:

XGBoost:

(mtry = 5, trees = 21)

0.685, 0.895, 0.877 with same as linear model
0.715, 0.891, 0.898 with previous campaigns
0.746, 0.882, 0.905, 0.864 with comms preference (tuned to tokens = 3)
0.748, 0.884, 0.905, 0.865 with previous source 2 channels (tuned to tokens = 20) *removed*
0.756, 0.895, 0.911, 0.876 with previous non financial actions


```{r}
train %>% str()
```


```{r}
cv %>% 
  autoplot()
```


```{r}
xgb_rec %>% prep() %>% bake(new_data = NULL)
```





















